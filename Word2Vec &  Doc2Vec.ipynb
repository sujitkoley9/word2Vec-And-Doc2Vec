{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Importing required packages\n",
    "\n",
    "1. https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec (i.e Hidden layer weight vector)\n",
    "<img src=\"images/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram.png\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "<br>\n",
    "1. <b>CBOW:</b>\n",
    "<img src=\"images/word2vec-cbow.png\" style=\"width:300px;height:300px;\">\n",
    "  To calculate hidden layer inputs, we take an average over all these C context word inputs.\n",
    "\n",
    "<br><br><br>\n",
    "2. <b>Skip Gram: </b>\n",
    "<img src=\"images/skip gram.png\" style=\"width:300px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "1. size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "2. window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "3. min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "4. workers: (default 3) The number of threads to use while training.\n",
    "5. sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "6. alpha: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1,size=100)\n",
    "# summarize the loaded model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.96484635e-04  4.02896106e-03  2.50628381e-03  1.13368430e-03\n",
      "  3.31014791e-03  2.36928416e-03  2.08337931e-03 -1.25981605e-05\n",
      " -4.20993241e-03  4.76118876e-03  3.67753417e-03 -3.19461571e-04\n",
      "  1.22230710e-03 -4.79406165e-03  1.38692011e-03 -2.93617068e-05\n",
      "  4.72569931e-03  2.90702539e-03  1.47349841e-03  3.98808764e-03\n",
      "  1.94128056e-03  1.59068673e-03  1.49845739e-03 -2.66237883e-03\n",
      " -2.40131398e-03 -4.74941451e-03 -4.06561838e-03  3.73138255e-03\n",
      "  3.93687282e-03  2.99954298e-03 -1.99720403e-03  9.32016002e-04\n",
      " -5.66960429e-04  1.47769402e-03 -3.52450064e-03  2.73211347e-03\n",
      " -7.41784577e-04  2.65987171e-03 -9.70018504e-04 -3.63929733e-03\n",
      " -2.59269588e-03  2.33549019e-03  1.08449801e-03 -1.54980796e-03\n",
      "  1.86904171e-03  3.93600576e-03  1.86595472e-03 -2.64461990e-03\n",
      "  2.66823132e-04  4.03169636e-03 -5.11152437e-04  4.30564582e-03\n",
      " -2.34517525e-03 -4.25397139e-03  6.17238693e-04  3.25139030e-03\n",
      " -4.74330457e-03 -3.65862716e-03 -1.21435220e-03  4.49695019e-03\n",
      " -3.46390298e-03  3.17979720e-03  3.33498872e-04 -8.86574504e-04\n",
      "  3.75183858e-03  3.12325265e-03  4.33716271e-03 -1.12988392e-03\n",
      "  7.01093464e-04  2.20005075e-03 -1.37483957e-03  2.34925188e-03\n",
      " -3.43965529e-03 -2.81856861e-03 -2.32486892e-03 -4.70475666e-03\n",
      "  1.94370165e-03  3.32551892e-03  1.83655310e-03 -3.67795280e-03\n",
      "  3.93282482e-03  1.80617091e-03 -5.43240341e-04  4.59600287e-03\n",
      " -3.25262430e-03 -3.58343800e-03  1.50914618e-03  5.08684840e-04\n",
      "  3.14412639e-03 -1.66667870e-03 -4.00449894e-03 -1.85988843e-03\n",
      " -9.09590279e-04 -3.06897960e-03  3.43584456e-03  2.42838589e-03\n",
      " -2.86340318e-03  3.42103979e-03  3.98844806e-03 -1.32187642e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujit koley\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print(model['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save(\"wordvectors.kv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
