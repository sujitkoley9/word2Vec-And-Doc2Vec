{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Importing required packages\n",
    "\n",
    "1. https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec (i.e Hidden layer weight vector)\n",
    "<img src=\"images/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram.png\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "<br>\n",
    "1. <b>CBOW:</b>\n",
    "<img src=\"images/word2vec-cbow.png\" style=\"width:300px;height:300px;\">\n",
    "  To calculate hidden layer inputs, we take an average over all these C context word inputs.\n",
    "\n",
    "<br><br><br>\n",
    "2. <b>Skip Gram: </b>\n",
    "<img src=\"images/skip gram.png\" style=\"width:300px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "1. size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "2. window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "3. min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "4. workers: (default 3) The number of threads to use while training.\n",
    "5. sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "6. alpha: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1,size=100)\n",
    "# summarize the loaded model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.96484635e-04  4.02896106e-03  2.50628381e-03  1.13368430e-03\n",
      "  3.31014791e-03  2.36928416e-03  2.08337931e-03 -1.25981605e-05\n",
      " -4.20993241e-03  4.76118876e-03  3.67753417e-03 -3.19461571e-04\n",
      "  1.22230710e-03 -4.79406165e-03  1.38692011e-03 -2.93617068e-05\n",
      "  4.72569931e-03  2.90702539e-03  1.47349841e-03  3.98808764e-03\n",
      "  1.94128056e-03  1.59068673e-03  1.49845739e-03 -2.66237883e-03\n",
      " -2.40131398e-03 -4.74941451e-03 -4.06561838e-03  3.73138255e-03\n",
      "  3.93687282e-03  2.99954298e-03 -1.99720403e-03  9.32016002e-04\n",
      " -5.66960429e-04  1.47769402e-03 -3.52450064e-03  2.73211347e-03\n",
      " -7.41784577e-04  2.65987171e-03 -9.70018504e-04 -3.63929733e-03\n",
      " -2.59269588e-03  2.33549019e-03  1.08449801e-03 -1.54980796e-03\n",
      "  1.86904171e-03  3.93600576e-03  1.86595472e-03 -2.64461990e-03\n",
      "  2.66823132e-04  4.03169636e-03 -5.11152437e-04  4.30564582e-03\n",
      " -2.34517525e-03 -4.25397139e-03  6.17238693e-04  3.25139030e-03\n",
      " -4.74330457e-03 -3.65862716e-03 -1.21435220e-03  4.49695019e-03\n",
      " -3.46390298e-03  3.17979720e-03  3.33498872e-04 -8.86574504e-04\n",
      "  3.75183858e-03  3.12325265e-03  4.33716271e-03 -1.12988392e-03\n",
      "  7.01093464e-04  2.20005075e-03 -1.37483957e-03  2.34925188e-03\n",
      " -3.43965529e-03 -2.81856861e-03 -2.32486892e-03 -4.70475666e-03\n",
      "  1.94370165e-03  3.32551892e-03  1.83655310e-03 -3.67795280e-03\n",
      "  3.93282482e-03  1.80617091e-03 -5.43240341e-04  4.59600287e-03\n",
      " -3.25262430e-03 -3.58343800e-03  1.50914618e-03  5.08684840e-04\n",
      "  3.14412639e-03 -1.66667870e-03 -4.00449894e-03 -1.85988843e-03\n",
      " -9.09590279e-04 -3.06897960e-03  3.43584456e-03  2.42838589e-03\n",
      " -2.86340318e-03  3.42103979e-03  3.98844806e-03 -1.32187642e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sujit koley\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print(model['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save(\"wordvectors.kv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Doc2Vec (i.e Average of wordvec of a sentence)\n",
    "\n",
    "<img src=\"images/Doc2Vec.png\" style=\"width:500px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1  Different Parameters\n",
    "\n",
    "1. <b>documents:</b>  Iterable of list of TaggedDocument\n",
    "2. <b>dm:</b> If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of words approach, which doesn't preserve any word order.\n",
    "3. <b>vector_size:</b> Dimensional feature vectors.\n",
    "4. <b>min_count:</b> Ignores all words with total frequency lower than this.\n",
    "5. <b>window:</b> The maximum distance between the current and predicted word within a sentence.\n",
    "6. <b>alpha:</b> The initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(tagged_data, dm=1,vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"d2v.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [-0.07063764 -0.06617451  0.00726097 -0.00809091  0.06903025]\n",
      "[('2', 0.4644005298614502), ('3', 0.11856015026569366), ('0', -0.5672831535339355)]\n",
      "[-0.04921347 -0.0373654   0.07231769 -0.07449533  0.05500709]\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
